\chapterimage{ss-controllers.jpg}{Night sky above Dufour Street in Santa Cruz, CA}

\chapter{State-space controllers}

When we want to command a \gls{system} to a set of states, we design a
controller with certain \glspl{control law} to do it. PID controllers use the
system outputs with proportional, integral, and derivative \glspl{control law}.
In state-space, we also have knowledge of the system states so we can do better.

\section{From PID to model-based control}
\index{PID control}

As mentioned before, controls engineers have a more general framework to
describe control theory than just PID control. PID controller designers are
focused on fiddling with controller parameters relating to the current, past,
and future error rather than the underlying system states. Integral control is a
commonly used tool, and some people use integral action as the majority of the
control action. While this approach works in a lot of situations, it is an
incomplete view of the world.

Model-based control has a completely different mindset. Controls designers using
model-based control care about developing an accurate model of the system, then
driving the states they care about to zero (or to a \gls{reference}). Integral
control is added with $u_{error}$ estimation if needed to handle model
uncertainty, but we prefer not to use it because its response is hard to tune
and some of its destabilizing dynamics aren't visible during simulation.

\section{Closed-loop controller}

With the \gls{control law} $\mtx{u} = \mtx{K}(\mtx{r} - \mtx{x})$, we can derive
the closed-loop state-space equations. We'll discuss where this control law
comes from in subsection \ref{sec:lqr}.

First is the state update equation. Substitute the control law into equation
(\ref{eq:ss_ctrl_x}).

\begin{align}
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{K}(\mtx{r} - \mtx{x}) \nonumber
    \\
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{K}\mtx{r} -
    \mtx{B}\mtx{K}\mtx{x} \nonumber \\
  \dot{\mtx{x}} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x} + \mtx{B}\mtx{K}\mtx{r}
\end{align}

Now for the output equation. Substitute the control law into equation
(\ref{eq:ss_ctrl_y}).

\begin{align}
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}(\mtx{K}(\mtx{r} - \mtx{x})) \nonumber \\
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}\mtx{K}\mtx{r} - \mtx{D}\mtx{K}\mtx{x}
    \nonumber \\
  \mtx{y} &= (\mtx{C} - \mtx{D}\mtx{K})\mtx{x} + \mtx{D}\mtx{K}\mtx{r}
\end{align}

\begin{theorem}[Closed-loop state-space controller]
  \index{State-space controllers!closed-loop}

  \begin{align}
    \dot{\mtx{x}} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x} + \mtx{B}\mtx{K}\mtx{r}
      \label{eq:s_ref_ctrl_x} \\
    \mtx{y} &= (\mtx{C} - \mtx{D}\mtx{K})\mtx{x} + \mtx{D}\mtx{K}\mtx{r}
      \label{eq:s_ref_ctrl_y}
  \end{align}

  \begin{figurekey}
    \begin{tabulary}{\linewidth}{LLLL}
      $\mtx{A}$ & system matrix      & $\mtx{K}$ & controller gain matrix \\
      $\mtx{B}$ & input matrix       & $\mtx{x}$ & state vector \\
      $\mtx{C}$ & output matrix      & $\mtx{r}$ & \gls{reference} vector \\
      $\mtx{D}$ & feedthrough matrix & $\mtx{y}$ & output vector \\
    \end{tabulary}
  \end{figurekey}
\end{theorem}

\begin{booktable}
  \begin{tabular}{|ll|ll|}
    \hline
    \rowcolor{headingbg}
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} &
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} \\
    \hline
    $\mtx{A}$ & states $\times$ states & $\mtx{x}$ & states $\times$ 1 \\
    $\mtx{B}$ & states $\times$ inputs & $\mtx{u}$ & inputs $\times$ 1 \\
    $\mtx{C}$ & outputs $\times$ states & $\mtx{y}$ & outputs $\times$ 1 \\
    $\mtx{D}$ & outputs $\times$ inputs & $\mtx{r}$ & states $\times$ 1 \\
    $\mtx{K}$ & inputs $\times$ states &  &  \\
    \hline
  \end{tabular}
  \caption{Controller matrix dimensions}
  \label{tab:ctrl_matrix_dims}
\end{booktable}

Instead of commanding the system to a state using the vector $\mtx{u}$ directly,
we can now specify a vector of desired states through $\mtx{r}$ and the system
will choose values of $\mtx{u}$ for us over time to make the system converge to
the reference. The rate of convergence and stability of the closed-loop system
can be changed by moving the poles via the eigenvalues of $\mtx{A} -
\mtx{B}\mtx{K}$. $\mtx{A}$ and $\mtx{B}$ are inherent to the system, but
$\mtx{K}$ can be chosen arbitrarily by the controller designer.

\subsection{Eigenvalues of closed-loop controller}
\index{Stability!eigenvalues}

The eigenvalues of the system matrix can be used to determine the stability of a
\gls{system}.

We'd like to know whether the \gls{system} defined by equation
(\ref{eq:ssz_ctrl_x}) operating with the \gls{control law}
$\mtx{u}_k = \mtx{K}(\mtx{r}_k - \mtx{x}_k)$ converges to the \gls{reference}
$\mtx{r}_k$.

\begin{align}
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k \nonumber \\
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}(\mtx{K}(\mtx{r}_k - \mtx{x}_k))
    \nonumber \\
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{K}\mtx{r}_k -
    \mtx{B}\mtx{K}\mtx{x}_k \nonumber \\
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k - \mtx{B}\mtx{K}\mtx{x}_k +
    \mtx{B}\mtx{K}\mtx{r}_k \nonumber \\
  \mtx{x}_{k+1} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x}_k +
    \mtx{B}\mtx{K}\mtx{r}_k \label{eq:ctrl_eig_calc}
\end{align}

For equation (\ref{eq:ctrl_eig_calc}) to have a bounded output, the eigenvalues
of $\mtx{A} - \mtx{B}\mtx{K}$ must be within the unit circle.

\section{Pole placement}
\index{Controller design!pole placement}

This is the practice of manually placing the poles of the closed-loop system to
produce a desired response. This is typically done with controllable canonical
form (see section \ref{sec:ctrl-canon}). This can be done with state observers
as well with observable canonical form (see section \ref{sec:obsv-canon}).

\section{LQR} \label{sec:lqr}
\index{Controller design!LQR}
\index{Optimal control!LQR}

While we could place the poles for our controller manually, we can do better by
using math to select them for us. ``LQR" stands for ``Linear-Quadratic
\glslink{regulator}{Regulator}". This method of controller design uses a
quadratic function for the cost-to-go defined as the sum of the error and
control effort over time for the linear system
$\dot{\mtx{x}} = \mtx{A}\mtx{x} + \mtx{B}\mtx{u}$.

\begin{equation*}
  J = \int\limits_0^\infty \left(\mtx{x}^T\mtx{Q}\mtx{x} +
    \mtx{u}^T\mtx{R}\mtx{u}\right) dt
\end{equation*}

where $J$ represents a tradeoff between \gls{state} excursion and control effort
with the weighting factors $\mtx{Q}$ and $\mtx{R}$. LQR finds a control law
$\mtx{u}$ that minimizes the cost function. $\mtx{Q}$ and $\mtx{R}$ slide the
cost along a Pareto boundary between state tracking and control effort (see
figure \ref{fig:pareto_boundary}). Pareto optimality for this problem means that
an improvement in state tracking cannot be obtained without using more control
effort to do so. Also, a reduction in control effort cannot be obtained without
sacrificing state tracking performance. Pole placement, on the other hand, will
have a cost anywhere on, above, or to the right of the Pareto boundary (no cost
can be inside the boundary).

\begin{svg}{build/code/pareto_boundary}
  \caption{Pareto boundary for LQR}
  \label{fig:pareto_boundary}
\end{svg}

The minimum of LQR's cost function is found by setting the derivative of the
cost function to zero and solving for the control law $\mtx{u}$. However, matrix
calculus is used instead of normal calculus to take the derivative.

The feedback \gls{control law} that minimizes $J$, which we'll call the
``optimal control law", is shown in theorem \ref{thm:optimal_control_law}.

\begin{theorem}[Optimal control law]
  \begin{equation}
    \mtx{u} = -\mtx{K}\mtx{x}
  \end{equation}
  \label{thm:optimal_control_law}
\end{theorem}
\index{Optimal control!LQR!optimal control law}

This means that optimal control can be achieved with simply a set of
proportional gains on all the states. This \gls{control law} will make all
states converge to zero assuming the system is controllable. To converge to
nonzero states, a reference vector $\mtx{r}$ can be added to the state
$\mtx{x}$.

\begin{theorem}[Optimal control law with nonzero reference]
  \begin{equation}
    \mtx{u} = \mtx{K}(\mtx{r} - \mtx{x})
  \end{equation}
\end{theorem}

To use the control law, we need knowledge of the full state of the system. That
means we either have to measure all our states directly or estimate those we do
not measure.

See appendix \ref{sec:deriv-optimal-control-law} for how $\mtx{K}$ is calculated
in Python. If the result is finite, the controller is guaranteed to be stable
and \glslink{robustness}{robust} with a \gls{phase margin} of 60 degrees
\cite{bib:lqr-phase-margin}.

\subsection{Bryson's rule}
\index{Optimal control!LQR!Bryson's rule}

The next obvious question is what values to choose for $\mtx{Q}$ and $\mtx{R}$.
With Bryson's rule, the diagonals of the $\mtx{Q}$ and $\mtx{R}$ matrices are
chosen based on the maximum acceptable value for each \gls{state} and actuator.
The nondiagonal elements are zero. The balance between $\mtx{Q}$ and $\mtx{R}$
can be slid along the Pareto boundary using a weighting factor $\rho$.

\begin{equation*}
  J = \int\limits_0^\infty \left(\rho \left[
    \left(\frac{x_1}{x_{1,max}}\right)^2 + \ldots +
    \left(\frac{x_n}{x_{n,max}}\right)^2\right] + \left[
    \left(\frac{u_1}{u_{1,max}}\right)^2 + \ldots +
    \left(\frac{u_n}{u_{n,max}}\right)^2\right]\right) dt
\end{equation*}

\begin{equation*}
  \begin{array}{cc}
    \mtx{Q} = \begin{bmatrix}
      \frac{\rho}{x_{1,max}^2} & 0 & \ldots & 0 \\
      0 & \frac{\rho}{x_{2,max}^2} & & \vdots \\
      \vdots & & \ddots & 0 \\
      0 & \ldots & 0 & \frac{\rho}{x_{n,max}^2}
    \end{bmatrix} &
    \mtx{R} = \begin{bmatrix}
      \frac{1}{u_{1,max}^2} & 0 & \ldots & 0 \\
      0 & \frac{1}{u_{2,max}^2} & & \vdots \\
      \vdots & & \ddots & 0 \\
      0 & \ldots & 0 & \frac{1}{u_{n,max}^2}
    \end{bmatrix}
  \end{array}
\end{equation*}

Small values of $\rho$ penalize control effort while large values of $\rho$
penalize \gls{state} excursions. Large values would be chosen in applications
like fighter jets where performance is necessary. Spacecrafts would use small
values to conserve their limited fuel supply.

\section{Localization}

State-space observers are used to estimate \glspl{state} which cannot be
measured directly. This can be due to noisy measurements or the state not being
measurable (a hidden state). This information can be used for
\gls{localization}, which is the process of using external measurements to
determine an \gls{agent}'s pose\footnote{An agent is a system-agnostic term for
independent controlled actors like robots or aircraft.}, or orientation in the
world.

One type of state estimator is LQE. ``LQE" stands for ``Linear-Quadratic
Estimator". Similar to LQR, it places the estimator poles such that it minimizes
the sum of squares of the error. The Luenberger observer and Kalman filter are
examples of these.

Computer vision can also be used for \gls{localization}. By extracting features
from an image taken by the \gls{agent}'s camera, like a retroreflective target
in FRC, and comparing them to known dimensions, one can determine where the
\gls{agent}'s camera would have to be to see that image. This can be used to
correct your state estimate in the same way we do with an encoder or gyroscope.

\subsection{Luenberger observer}
\index{State-space observers!Luenberger observer}

\begin{theorem}[Luenberger observer]
  \begin{align}
    \dot{\hat{\mtx{x}}} &= \mtx{A}\hat{\mtx{x}} + \mtx{B}\mtx{u} +
      \mtx{L} (\mtx{y} - \hat{\mtx{y}}) \label{eq:s_obsv_x} \\
    \hat{\mtx{y}} &= \mtx{C}\hat{\mtx{x}} + \mtx{D}\mtx{u} \label{eq:s_obsv_y}
  \end{align}

  \begin{align}
    \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
      \mtx{L}(\mtx{y}_k - \hat{\mtx{y}}_k) \label{eq:z_obsv_x} \\
    \hat{\mtx{y}}_k &= \mtx{C}\hat{\mtx{x}}_k + \mtx{D}\mtx{u}_k
      \label{eq:z_obsv_y} \\ \nonumber
  \end{align}

  \begin{figurekey}
    \begin{tabulary}{\linewidth}{LLLL}
      $\mtx{A}$ & system matrix      & $\hat{\mtx{x}}$ & state estimate vector \\
      $\mtx{B}$ & input matrix       & $\mtx{u}$ & input vector \\
      $\mtx{C}$ & output matrix      & $\mtx{y}$ & output vector \\
      $\mtx{D}$ & feedthrough matrix & $\hat{\mtx{y}}$ & output estimate vector \\
      $\mtx{L}$ & estimator gain matrix & & \\
    \end{tabulary}
  \end{figurekey}
\end{theorem}

\begin{booktable}
  \begin{tabular}{|ll|ll|}
    \hline
    \rowcolor{headingbg}
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} &
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} \\
    \hline
    $\mtx{A}$ & states $\times$ states & $\hat{\mtx{x}}$ & states $\times$ 1 \\
    $\mtx{B}$ & states $\times$ inputs & $\mtx{u}$ & inputs $\times$ 1 \\
    $\mtx{C}$ & outputs $\times$ states & $\mtx{y}$ & outputs $\times$ 1 \\
    $\mtx{D}$ & outputs $\times$ inputs & $\hat{\mtx{y}}$ & outputs $\times$ 1 \\
    $\mtx{L}$ & states $\times$ outputs & & \\
    \hline
  \end{tabular}
  \caption{Luenberger observer matrix dimensions}
  \label{tab:luenberger_matrix_dims}
\end{booktable}

Variables denoted with a hat are estimates of the corresponding variable. For
example, $\hat{\mtx{x}}$ is the estimate of the true state $\mtx{x}$.

Notice that a Luenberger observer has an extra term in the state evolution
equation. This term uses the difference between the estimated outputs and
measured outputs to steer the estimated state toward the true state. Large
values of $\mtx{L}$ trust the measurements more while small values trust the
model more.

\begin{remark}
  Using an estimator forfeits the performance guarantees from earlier, but the
  responses are still generally very good if the process and measurement noises
  are small enough. See John Doyle's paper \textit{Guaranteed Margins for LQG
  Regulators} for a proof.
\end{remark}

A Luenberger observer combines the prediction and update steps of an estimator.
To run them separately, use the equations in theorem \ref{thm:luenberger}
instead.

\begin{theorem}[Luenberger observer with separate predict/update]
  \begin{align}
    \text{Predict step} \nonumber \\
    \hat{\mtx{x}}_{k+1}^- &= \mtx{A}\hat{\mtx{x}}_k^- + \mtx{B}\mtx{u}_k \\
    \text{Update step} \nonumber \\
    \hat{\mtx{x}}_{k+1}^+ &= \hat{\mtx{x}}_{k+1}^- + \mtx{A}^{-1}\mtx{L}
      (\mtx{y}_k - \hat{\mtx{y}}_k) \\
    \hat{\mtx{y}}_k &= \mtx{C} \hat{\mtx{x}}_k^-
  \end{align}
  \label{thm:luenberger}
\end{theorem}

See appendix \ref{subsec:deriv-luenberger-separate} for a derivation.

\subsubsection{Eigenvalues of closed-loop observer}
\index{Stability!eigenvalues}

The eigenvalues of the system matrix can be used to determine whether a
\gls{state} observer's estimate will converge to the true \gls{state}.

Plugging equation (\ref{eq:z_obsv_y}) into equation (\ref{eq:z_obsv_x}) gives

\begin{align*}
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - \hat{\mtx{y}}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - (\mtx{C}\hat{\mtx{x}}_k + \mtx{D}\mtx{u}_k)) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - \mtx{C}\hat{\mtx{x}}_k - \mtx{D}\mtx{u}_k)
\end{align*}

Plugging in equation (\ref{eq:ssz_ctrl_y}) gives

\begin{align*}
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}((\mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k) - \mtx{C}\hat{\mtx{x}}_k -
    \mtx{D}\mtx{u}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}(\mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k - \mtx{C}\hat{\mtx{x}}_k -
    \mtx{D}\mtx{u}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}(\mtx{C}\mtx{x}_k - \mtx{C}\hat{\mtx{x}}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}\mtx{C}(\mtx{x}_k - \hat{\mtx{x}}_k)
\end{align*}

Let $E_k = \mtx{x}_k - \hat{\mtx{x}}_k$ be the error in the estimate
$\hat{\mtx{x}}_k$.

\begin{equation*}
  \hat{\mtx{x}}_{k+1} = \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}\mtx{C}\mtx{E}_k
\end{equation*}

Subtracting this from equation (\ref{eq:ssz_ctrl_x}) gives

\begin{align}
  \mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k -
    (\mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
     \mtx{L}\mtx{C}\mtx{E}_k) \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k -
    (\mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k + \mtx{L}\mtx{C}\mtx{E}_k)
    \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k -
    \mtx{A}\hat{\mtx{x}}_k - \mtx{B}\mtx{u}_k - \mtx{L}\mtx{C}\mtx{E}_k
    \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{x}_k - \mtx{A}\hat{\mtx{x}}_k -
    \mtx{L}\mtx{C}\mtx{E}_k \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}(\mtx{x}_k - \hat{\mtx{x}}_k) -
    \mtx{L}\mtx{C}\mtx{E}_k \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{E}_k - \mtx{L}\mtx{C}\mtx{E}_k \nonumber \\
  \mtx{E}_{k+1} &= (\mtx{A} - \mtx{L}\mtx{C})\mtx{E}_k \label{eq:obsv_eig_calc}
\end{align}

For equation (\ref{eq:obsv_eig_calc}) to have a bounded output, the eigenvalues
of $\mtx{A} - \mtx{L}\mtx{C}$ must be within the unit circle. These eigenvalues
represent how fast the estimator converges to the true state of the given
\gls{model}. A fast estimator converges quickly while a slow estimator avoids
amplifying noise in the measurements used to produce a state estimate.

As stated before, the controller and estimator are dual problems. Controller
gains can be found assuming perfect estimator (i.e., perfect knowledge of all
\glspl{state}). Estimator gains can be found assuming an accurate \gls{model}
and a controller with perfect \gls{tracking}.

The effect of noise can be seen if it is modeled
\glslink{stochastic process}{stochastically} as

\begin{equation*}
  \hat{\mtx{x}}_{k+1} = \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} ((\mtx{y}_k + \mtx{\nu}_k) - \hat{\mtx{y}}_k) \\
\end{equation*}

where $\mtx{\nu}_k$ is the measurement noise. Rearranging this equation yields

\begin{align*}
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - \hat{\mtx{y}}_k + \mtx{\nu}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - \hat{\mtx{y}}_k) + \mtx{L}\mtx{\nu}_k
\end{align*}

As $\mtx{L}$ increases, the measurement noise is amplified.
