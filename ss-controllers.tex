\chapterimage{ss-controllers.jpg}{OPERS field at UCSC}

\chapter{State-space controllers}

When we want to command a \gls{system} to a set of states, we design a
controller with certain \glspl{control law} to do it. PID controllers use the
system outputs with proportional, integral, and derivative \glspl{control law}.
In state-space, we also have knowledge of the system states so we can do better.

\section{From PID to model-based control}
\index{PID control}

As mentioned before, controls engineers have a more general framework to
describe control theory than just PID control. PID controller designers are
focused on fiddling with controller parameters relating to the current, past,
and future error rather than the underlying system states. Integral control is a
commonly used tool, and some people use integral action as the majority of the
control action. While this approach works in a lot of situations, it is an
incomplete view of the world.

Model-based control has a completely different mindset. Controls designers using
model-based control care about developing an accurate model of the system, then
driving the states they care about to zero (or to a \gls{reference}). Integral
control is added with $u_{error}$ estimation if needed to handle model
uncertainty, but we prefer not to use it because its response is hard to tune
and some of its destabilizing dynamics aren't visible during simulation.

\section{Closed-loop controller}

With the \gls{control law} $\mtx{u} = \mtx{K}(\mtx{r} - \mtx{x})$, we can derive
the closed-loop state-space equations. We'll discuss where this control law
comes from in subsection \ref{subsec:pareto_optimal_curve}.

First is the state update equation. Substitute the control law into equation
(\ref{eq:ss_ctrl_x}).

\begin{align}
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{K}(\mtx{r} - \mtx{x}) \nonumber
    \\
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{K}\mtx{r} -
    \mtx{B}\mtx{K}\mtx{x} \nonumber \\
  \dot{\mtx{x}} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x} + \mtx{B}\mtx{K}\mtx{r}
\end{align}

Now for the output equation. Substitute the control law into equation
(\ref{eq:ss_ctrl_y}).

\begin{align}
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}(\mtx{K}(\mtx{r} - \mtx{x})) \nonumber \\
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}\mtx{K}\mtx{r} - \mtx{D}\mtx{K}\mtx{x}
    \nonumber \\
  \mtx{y} &= (\mtx{C} - \mtx{D}\mtx{K})\mtx{x} + \mtx{D}\mtx{K}\mtx{r}
\end{align}

\begin{theorem}[Closed-loop state-space controller]
  \begin{align}
    \dot{\mtx{x}} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x} + \mtx{B}\mtx{K}\mtx{r}
      \label{eq:s_ref_ctrl_x} \\
    \mtx{y} &= (\mtx{C} - \mtx{D}\mtx{K})\mtx{x} + \mtx{D}\mtx{K}\mtx{r}
      \label{eq:s_ref_ctrl_y}
  \end{align}

  \begin{figurekey}
    \begin{tabulary}{\linewidth}{LLLL}
      $\mtx{A}$ & system matrix      & $\mtx{x}$ & state vector \\
      $\mtx{B}$ & input matrix       & $\mtx{u}$ & input vector \\
      $\mtx{C}$ & output matrix      & $\mtx{y}$ & output vector \\
      $\mtx{D}$ & feedthrough matrix & $\mtx{r}$ & \gls{reference} vector \\
      $\mtx{K}$ & controller gain matrix &  &  \\
    \end{tabulary}
  \end{figurekey}
\end{theorem}

\begin{booktable}
  \begin{tabular}{|ll|ll|}
    \hline
    \rowcolor{headingbg}
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} &
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} \\
    \hline
    $\mtx{A}$ & states $\times$ states & $\mtx{x}$ & states $\times$ 1 \\
    $\mtx{B}$ & states $\times$ inputs & $\mtx{u}$ & inputs $\times$ 1 \\
    $\mtx{C}$ & outputs $\times$ states & $\mtx{y}$ & outputs $\times$ 1 \\
    $\mtx{D}$ & outputs $\times$ inputs & $\mtx{r}$ & states $\times$ 1 \\
    $\mtx{K}$ & inputs $\times$ states &  &  \\
    \hline
  \end{tabular}
  \caption{Controller matrix dimensions}
  \label{tab:ctrl_matrix_dims}
\end{booktable}

Instead of commanding the system to a state using the vector $\mtx{u}$ directly,
we can now specify a vector of desired states through $\mtx{r}$ and the system
will choose values of $\mtx{u}$ for us over time to make the system converge to
the reference. The rate of convergence and stability of the closed-loop system
can be changed by moving the poles via the eigenvalues of $\mtx{A} -
\mtx{B}\mtx{K}$. $\mtx{A}$ and $\mtx{B}$ are inherent to the system, but
$\mtx{K}$ can be chosen arbitrarily by the controller designer.

\section{Pole placement}
\index{Controller design!pole placement}

This is the practice of manually placing the poles of the closed-loop system to
produce a desired response. This is typically done with controllable canonical
form (see section \ref{sec:ctrl-canon}). This can be done with state observers
as well with observable canonical form (see section \ref{sec:obsv-canon}).

\section{LQR}
\index{Controller design!LQR}
\index{Optimal control!LQR}

While we could place the poles for our controller manually, we can do better by
using math to select them for us. ``LQR" stands for ``Linear-Quadratic
\glslink{regulator}{Regulator}". This method of controller design uses a
quadratic function for the cost-to-go defined as the sum of the error and
control effort over time for the linear system. LQR places the poles of the
controller such that this cost function is minimized.

The minimum of LQR's cost function is found by setting the derivative of the
cost function to zero and solving for the control law $\mtx{u}$. However, matrix
calculus is used instead of normal calculus to take the derivative.

Next, we'll define the cost function used by LQR and offer some advice for
tuning its parameters.

\subsection{Pareto optimal curve} \label{subsec:pareto_optimal_curve}

A system $\dot{\mtx{x}} = \mtx{A}\mtx{x} + \mtx{B}\mtx{u}$ has the cost function

\begin{equation*}
  J = \int\limits_0^\infty \left(\mtx{x}^T\mtx{Q}\mtx{x} +
    \mtx{u}^T\mtx{R}\mtx{u}\right) dt
\end{equation*}

where $J$ represents a tradeoff between \gls{state} excursion and control effort
with the weighting factors $\mtx{Q}$ and $\mtx{R}$. $\mtx{Q}$ and $\mtx{R}$
slide the cost along a Pareto optimal curve between state tracking and control
effort (see figure \ref{fig:pareto_curve}). This means that an improvement in
state tracking cannot be obtained without using more control effort to do so.
Also, a reduction in control effort cannot be obtained without sacrificing state
tracking performance. Pole placement, on the other hand, will have a cost
anywhere on or above/to the right of the Pareto optimal curve (no cost can be
inside the curve).

\begin{svg}{build/code/pareto_curve}
  \caption{Pareto optimal curve for LQR}
  \label{fig:pareto_curve}
\end{svg}

The feedback \gls{control law} that minimizes $J$, which we'll call the
``optimal control law", is shown in theorem \ref{thm:optimal_control_law}.

\begin{theorem}[Optimal control law]
  \begin{equation}
    \mtx{u} = -\mtx{K}\mtx{x}
  \end{equation}
  \label{thm:optimal_control_law}
\end{theorem}
\index{Optimal control!LQR!optimal control law}

This means that optimal control can be achieved with simply a set of
proportional gains on all the states. This \gls{control law} will make all
states converge to zero assuming the system is controllable. To converge to
nonzero states, a reference vector $\mtx{r}$ can be added to the state
$\mtx{x}$.

\begin{theorem}[Optimal control law with nonzero reference]
  \begin{equation}
    \mtx{u} = \mtx{K}(\mtx{r} - \mtx{x})
  \end{equation}
\end{theorem}

To use the control law, we need knowledge of the full state of the system. That
means we either have to measure all our states directly or estimate those we do
not measure.

See appendix \ref{ch:app-optimal-control-law-deriv} for how $\mtx{K}$ is
calculated in Python. If the result is finite, the controller is guaranteed to
be stable and \glslink{robustness}{robust} with a \gls{phase margin} of 60
degrees \cite{bib:lqr-phase-margin}.

\subsection{Bryson's rule}
\index{Optimal control!LQR!Bryson's rule}

The next obvious question is what values to choose for $\mtx{Q}$ and $\mtx{R}$.
With Bryson's rule, the $\mtx{Q}$ and $\mtx{R}$ matrices are chosen based on the
maximum acceptable value for each \gls{state} and actuator. The balance between
$\mtx{Q}$ and $\mtx{R}$ can be slid along the Pareto optimal curve using a
weighting factor $\rho$.

\begin{equation*}
  J = \int\limits_0^\infty \left(\rho \left[
    \left(\frac{x_1}{x_{1,max}}\right)^2 + \ldots +
    \left(\frac{x_n}{x_{n,max}}\right)^2\right] + \left[
    \left(\frac{u_1}{u_{1,max}}\right)^2 + \ldots +
    \left(\frac{u_n}{u_{n,max}}\right)^2\right]\right) dt
\end{equation*}

Small values of $\rho$ penalize control effort while large values of $\rho$
penalize \gls{state} excursions. Large values would be chosen in applications
like fighter jets where performance is necessary. Spacecrafts would use small
values to conserve their limited fuel supply.
