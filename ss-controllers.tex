\chapterimage{ss-controllers.jpg}{OPERS field at UCSC}

\chapter{State-space controllers}

When we want to command a \gls{system} to a set of states, we design a
controller with certain \glspl{control law} to do it. PID controllers use the
system outputs with proportional, integral, and derivative \glspl{control law}.
In state-space, we also have knowledge of the system states so we can do better.

\section{From PID to model-based control}

As mentioned before, controls engineers have a more general framework to
describe control theory than just PID control. PID controller designers are
focused on fiddling with controller parameters relating to the current, past,
and future error rather than the underlying system states. Integral control is a
commonly used tool, and some people use integral action as the majority of the
control action. While this approach works in a lot of situations, it is an
incomplete view of the world.

Model-based control has a completely different mindset. Controls designers using
model-based control care about developing an accurate model of the system, then
driving the states they care about to zero (or to a \gls{reference}). Integral
control is added with $u_{error}$ estimation if needed to handle model
uncertainty, but we prefer not to use it because its response is hard to tune
and some of its destabilizing dynamics aren't visible during simulation.

\section{Optimal control law}

The optimal \gls{control law} for a multiple-input, multiple-output system is
the following.

\begin{theorem}[Optimal control law]
  \begin{equation}
    \mtx{u} = -\mtx{K}\mtx{x}
  \end{equation}
\end{theorem}

This means that optimal control can be achieved with simply a set of
proportional gains on all the states. The reason for this will be explained when
discussing Linear-Quadratic Regulators.

This \gls{control law} will make all states converge to zero assuming the system
is controllable. To converge to nonzero states, a reference vector $\mtx{r}$ can
be added to the state $\mtx{x}$.

\begin{theorem}[Optimal control law with nonzero reference]
  \begin{equation}
    \mtx{u} = \mtx{K}(\mtx{r} - \mtx{x}) \label{eq:s_ref_ctrl_law}
  \end{equation}
\end{theorem}

\section{Controller}

Given the \gls{control law} above, we can derive the closed-loop state-space
equations.

First is the state update equation. Substitute equation
(\ref{eq:s_ref_ctrl_law}) into equation (\ref{eq:ss_ctrl_x}).

\begin{align}
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{K}(\mtx{r} - \mtx{x}) \nonumber
    \\
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{K}\mtx{r} -
    \mtx{B}\mtx{K}\mtx{x} \nonumber \\
  \dot{\mtx{x}} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x} + \mtx{B}\mtx{K}\mtx{r}
\end{align}

Now for the output equation. Substitute equation (\ref{eq:s_ref_ctrl_law}) into
equation (\ref{eq:ss_ctrl_y}).

\begin{align}
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}(\mtx{K}(\mtx{r} - \mtx{x})) \nonumber \\
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}\mtx{K}\mtx{r} - \mtx{D}\mtx{K}\mtx{x}
    \nonumber \\
  \mtx{y} &= (\mtx{C} - \mtx{D}\mtx{K})\mtx{x} + \mtx{D}\mtx{K}\mtx{r}
\end{align}

\begin{theorem}[Closed-loop state-space controller]
  \begin{align}
    \dot{\mtx{x}} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x} + \mtx{B}\mtx{K}\mtx{r}
      \label{eq:s_ref_ctrl_x} \\
    \mtx{y} &= (\mtx{C} - \mtx{D}\mtx{K})\mtx{x} + \mtx{D}\mtx{K}\mtx{r}
      \label{eq:s_ref_ctrl_y}
  \end{align}

  \begin{figurekey}
    \begin{tabulary}{\linewidth}{LLLL}
      $\mtx{A}$ & system matrix      & $\mtx{x}$ & state vector \\
      $\mtx{B}$ & input matrix       & $\mtx{u}$ & input vector \\
      $\mtx{C}$ & output matrix      & $\mtx{y}$ & output vector \\
      $\mtx{D}$ & feedthrough matrix & $\mtx{r}$ & \gls{reference} vector \\
      $\mtx{K}$ & controller gain matrix &  &  \\
    \end{tabulary}
  \end{figurekey}
\end{theorem}

\begin{booktable}
  \begin{tabular}{|ll|ll|}
    \hline
    \rowcolor{headingbg}
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} &
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} \\
    \hline
    $\mtx{A}$ & states $\times$ states & $\mtx{x}$ & states $\times$ 1 \\
    $\mtx{B}$ & states $\times$ inputs & $\mtx{u}$ & inputs $\times$ 1 \\
    $\mtx{C}$ & outputs $\times$ states & $\mtx{y}$ & outputs $\times$ 1 \\
    $\mtx{D}$ & outputs $\times$ inputs & $\mtx{r}$ & states $\times$ 1 \\
    $\mtx{K}$ & inputs $\times$ states &  &  \\
    \hline
  \end{tabular}
  \caption{Controller matrix dimensions}
  \label{tab:ctrl_matrix_dims}
\end{booktable}

Instead of commanding the system to a state using the vector $\mtx{u}$ directly,
we can now specify a vector of desired states through $\mtx{r}$ and the system
will choose values of $\mtx{u}$ for us over time to make the system converge to
the reference. The rate of convergence and stability of the closed-loop system
can be changed by moving the poles via the eigenvalues of $\mtx{A} -
\mtx{B}\mtx{K}$. $\mtx{A}$ and $\mtx{B}$ are inherent to the system, but
$\mtx{K}$ can be chosen arbitrarily by the controller designer.

\section{Need full state}

While we can attain good control with this approach, we need knowledge of the
full state of the system. That means we either have to measure all our states
directly or estimate those we do not.

\section{Pole placement}

This is the practice of manually placing the poles of the closed-loop system to
produce a desired response. This is typically done with controllable canonical
form (see subsubsection \ref{subsubsec:ctrl-canon}). This can be done with state
observers as well with observable canonical form (see subsubsection
\ref{subsubsec:obsv-canon}).

\section{LQR}

While we could place the poles for our controller manually, we can do better
using math to select them for us. ``LQR" stands for ``Linear-Quadratic
Regulator". It places the poles of the controller to find the minimum of a
quadratic cost function, which will be discussed below.

\subsection{Pareto optimal curve}

A system $\dot{\mtx{x}} = \mtx{A}\mtx{x} + \mtx{B}\mtx{u}$ has the cost function

\begin{equation*}
  J = \int\limits_0^\infty \left(\mtx{x}^T\mtx{Q}\mtx{x} +
    \mtx{u}^T\mtx{R}\mtx{u}\right) dt
\end{equation*}

where $J$ represents a tradeoff between \gls{state} excursion and control effort
with the weighting factors $\mtx{Q}$ and $\mtx{R}$. The feedback
\gls{control law} that minimizes $J$ is

\begin{equation*}
  \mtx{u} = -\mtx{K}\mtx{x}
\end{equation*}

See appendix \ref{ch:app-optimal-control-law-deriv} for how $\mtx{K}$ is
calculated in Python.

If the solution produces a finite value, the resulting controller is guaranteed
to be stable and \glslink{robustness}{robust} with a \gls{phase margin} of 55
degrees.

\subsection{Bryson's rule}

The next obvious question is what values to choose for $Q$ and $R$. With
Bryson's rule, the $Q$ and $R$ matrices are chosen based on the maximum
acceptable value for each \gls{state} and actuator. The balance between $Q$ and
$R$ can be slid along the Pareto optimal curve using a weighting factor $\rho$.

Small values of $\rho$ penalize \gls{state} excursions while large values of
$\rho$ penalize control effort. Small values would be chosen in applications
like fighter jets where performance is necessary. Spacecrafts would use large
values to conserve their limited fuel supply.
