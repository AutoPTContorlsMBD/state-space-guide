\chapterimage{digital-control.jpg}{Chaparral by Merril Apartments at UCSC}

\chapter{Digital control} \label{ch:digital_control}

The complex plane discussed so far deals with continuous \glspl{system}. In
decades past, \glspl{plant} and controllers were implemented using analog
electronics, which are continuous in nature. Nowadays, microprocessors can be
used to achieve cheaper, less complex controller designs.
\glslink{discretization}{Discretization} converts the continuous model we've
worked with so far from a set of differential equations like

\begin{equation}
  \dot{x} = x - 3 \label{eq:differential_equ_example}
\end{equation}

to a set of difference equations like

\begin{equation}
  x_{k+1} = x_k + (x_k - 3) \Delta T \label{eq:difference_equ_example}
\end{equation}

where the difference equation is run with some update period denoted by $T$, by
$\Delta T$, or sometimes sloppily by $dt$.

While higher order terms of a differential equation are derivatives of the state
variable (e.g., $\ddot{x}$ in relation to equation
(\ref{eq:differential_equ_example})), higher order terms of a difference
equation are delayed copies of the state variable (e.g., $x_{k-1}$ with respect
to $x_k$ in equation (\ref{eq:difference_equ_example})).

\section{Phase loss} \label{sec:phase_loss}

However, \gls{discretization} has drawbacks. Since a microcontroller performs
discrete steps, there is a sample delay that introduces phase loss in the
controller. Phase loss is the reduction of phase margin (see section
\ref{sec:gain-phase-margin}) that occurs in digital implementations of feedback
controllers from sampling the continuous system at discrete time intervals. As
the sample rate of the controller decreases, the phase margin decreases
according to $-\frac{T}{2}\omega$ where $T$ is the sample period and $\omega$ is
the frequency of the system dynamics. Instability occurs if the phase margin of
the system reaches zero. Large amounts of phase loss can make a stable
controller in the continuous domain become unstable in the discrete domain. Here
are a few ways to combat this.

\begin{itemize}
  \item Run the controller with a high sample rate.
  \item Designing the controller in the analog domain with enough phase margin
    to compensate for any phase loss that occurs as part of discretization.
  \item Convert the \gls{plant} to the digital domain and design the controller
    completely in the digital domain.
\end{itemize}

\section{s-plane to z-plane}

Transfer functions are converted to impulse responses using the Z-transform. The
s-plane's LHP maps to the inside of a unit circle in the z-plane. Table
\ref{tab:s2z_mapping} contains a few common points and figure
\ref{fig:s2z_mapping} shows the mapping visually.

\begin{booktable}
  \begin{tabular}{|cc|}
    \hline
    \rowcolor{headingbg}
    \textbf{s-plane} & \textbf{z-plane} \\
    \hline
    $(0, 0)$ & $(1, 0)$ \\
    imaginary axis & edge of unit circle \\
    $(-\infty, 0)$ & $(0, 0)$ \\
    \hline
  \end{tabular}
  \caption{Mapping from s-plane to z-plane}
  \label{tab:s2z_mapping}
\end{booktable}

\begin{figure}
  \begin{minisvg}{build/code/s_plane}
  \end{minisvg}
  \begin{minisvg}{build/code/z_plane}
  \end{minisvg}
  \caption{Mapping of axes from s-plane (left) to z-plane (right)}
  \label{fig:s2z_mapping}
\end{figure}

\subsection{z-plane stability}

Eigenvalues of a system that are within the unit circle are stable, but why is
that? Let's consider a scalar equation $x_{k + 1} = ax_k$. $a < 1$ makes
$x_{k + 1}$ converge to zero. The same applies to a complex number like
$z = x + yi$ for $x_{k + 1} = zx_k$. If the magnitude of the complex number $z$
is less than one, $x_{k+1}$ will converge to zero. Values with a magnitude of
$1$ oscillate forever because $x_{k+1}$ never decays.

\subsection{z-plane behavior}

As $\omega$ increases in $s = j\omega$, a pole in the z-plane moves around the
perimeter of the unit circle. Once it hits $\frac{\omega_s}{2}$ (half the
sampling frequency) at $(-1, 0)$, the pole wraps around. This is due to poles
faster than the sample frequency folding down to below the sample frequency
(that is, higher frequency signals \textit{alias} to lower frequency ones).

You may notice that poles can be placed at $(0, 0)$ in the z-plane. This is
known as a deadbeat controller. An $\rm N^{th}$ order deadbeat controller decays
to the \gls{reference} in N timesteps. While this sounds great, there are other
considerations like actuation effort, \gls{robustness}, and
\gls{noise immunity}. These will be discussed in more detail with LQR and LQE.

If poles from $(1, 0)$ to $(0, 0)$ on the x-axis approach infinity, then what do
poles from $(-1, 0)$ to $(0, 0)$ represent? Them being faster than infinity
doesn't make sense. Poles in this location exhibit oscillatory behavior similar
to complex conjugate pairs. See figures \ref{fig:z_oscillations_1p} and
\ref{fig:z_oscillations_2p}. The jaggedness of these signals is due to the
frequency of the system dynamics being above the Nyquist frequency. The
discretized signal doesn't have enough samples to reconstruct the continuous
system's dynamics.

\begin{figure}
  \begin{minisvg}{build/code/z_oscillations_1p}
    \caption{Single poles in various locations in z-plane}
    \label{fig:z_oscillations_1p}
  \end{minisvg}
  \hfill
  \begin{minisvg}{build/code/z_oscillations_2p}
    \caption{Complex conjugate poles in various locations in z-plane}
    \label{fig:z_oscillations_2p}
  \end{minisvg}
\end{figure}

\subsection{Nyquist frequency}
\index{Digital signal processing!Nyquist frequency}
\index{Digital signal processing!aliasing}

To completely reconstruct a signal, the Nyquist-Shannon sampling theorem states
that it must be sampled at a frequency at least twice the maximum frequency it
contains. The highest frequency a given sample rate can capture is called the
Nyquist frequency, which is half the sample frequency. This is why recorded
audio is sampled at $44.1k\,Hz$. The maximum frequency a typical human can hear
is about $20\,kHz$, so the Nyquist frequency is $40\,kHz$. ($44.1kHz$ in
particular was chosen for unrelated historical reasons.)

Frequencies above the Nyquist frequency are folded down across it. The higher
frequency and the folded down lower frequency are said to alias each
other\footnote{The aliases of a frequency $f$ can be expressed as
$f_{alias}(N) \stackrel{def}{=} |f - Nf_s|$. For example, if a $200\,Hz$ sine
wave is sampled at $150\,Hz$, the observer will see a $50\,Hz$ signal instead of
a $200\,Hz$ one.}. Figure \ref{fig:nyquist} provides a demonstration of
aliasing.

\begin{svg}{build/code/nyquist}
  \caption{The samples of two sine waves can be identical when at least one of
    them is at a frequency above half the sample rate. In this case, the $2\,Hz$
    sine wave is above the Nyquist frequency $1.5\,Hz$.}
  \label{fig:nyquist}
\end{svg}

The effect of these high-frequency aliases can be reduced with a low-pass filter
(called an anti-aliasing filter in this application).

\section{Discretization methods} \label{sec:discretization_methods}
\index{Discretization}

Discretization is done using a zero-order hold. That is, the system state is
only updated at discrete intervals and it's held constant between samples (see
figure \ref{fig:zoh}). The exact method of applying this uses the matrix
exponential, but this can be computationaly expensive. Instead, approximations
such as the following are used.

\begin{enumerate}
  \item Forward Euler method. This is defined as
    $y_{n+1} = y_n + f(t_n, y_n) \Delta t$.
    \index{Discretization!forward Euler method}
  \item Backward Euler method. This is defined as
    $y_{n+1} = y_n + f(t_{n+1}, y_{n+1}) \Delta t$.
    \index{Discretization!backward Euler method}
  \item Bilinear transform. The first-order bilinear approximation is
    $s = \frac{2}{T} \frac{1 - z^{-1}}{1 + z^{-1}}$.
    \index{Discretization!bilinear transform}
\end{enumerate}

where the function $f(t_n, y_n)$ is the slope of $y$ at $n$ and $T$ is the
sample period for the discrete system. Each of these methods is essentially
finding the area underneath a curve. The forward and backward Euler methods use
rectangles to approximate that area while the bilinear transform uses
trapezoids (see figures \ref{fig:discretization_methods_vel} and
\ref{fig:discretization_methods_pos}). Since these are approximations, there is
distortion between the real discrete system's poles and the approximate poles.
This is in addition to the phase loss introduced by discretizing at a given
sample rate in the first place. For fast-changing systems, this distortion can
quickly lead to instability.

\begin{svg}{build/code/zoh}
    \caption{Zero-order hold of a system response}
    \label{fig:zoh}
\end{svg}

\begin{svg}{build/code/discretization_methods_vel}
  \caption{Discretization methods applied to velocity data}
  \label{fig:discretization_methods_vel}
\end{svg}

\begin{svg}{build/code/discretization_methods_pos}
  \caption{Position plot of discretization methods applied to velocity data}
  \label{fig:discretization_methods_pos}
\end{svg}

\section{Effects of discretization on controller performance}

Running a feedback controller at a faster update rate doesn't always mean better
control. In fact, you may be using more computational resources than you need.
However, here are some reasons for running at a faster update rate.

Firstly, if you have a discrete model of the system, that model can more
accurately approximate the underlying continuous system. Most applications of
PID control don't have a model though.

Secondly, the controller can better handle fast system dynamics. If the system
can move from its initial state to the desired one in under 250ms, you obviously
want to run the controller with a period less than this. When you reduce the
sample period, you're making the discrete controller more accurately reflect
what the equivalent continuous controller would do (controllers built from
analog circuit components like op-amps are continuous).

Running at a lower sample rate only causes problems if you don't take into
account the response time of your system. Some systems like heaters have outputs
(output = measurement) that change on the order of minutes. Running a control
loop at 1kHz doesn't make sense for this because the plant input the controller
computes won't change much, if at all, in 1ms.

Figures \ref{fig:sampling_simulation_0.1}, \ref{fig:sampling_simulation_0.05},
and \ref{fig:sampling_simulation_0.04} show simulations of the same controller
for different sampling methods and sample rates, which have varying levels of
fidelity to the real system.

\begin{svg}{build/code/sampling_simulation_010}
  \caption{Sampling methods for system simulation with $T = 0.1s$}
  \label{fig:sampling_simulation_0.1}
\end{svg}

\begin{svg}{build/code/sampling_simulation_005}
  \caption{Sampling methods for system simulation with $T = 0.05s$}
  \label{fig:sampling_simulation_0.05}
\end{svg}

\begin{svg}{build/code/sampling_simulation_004}
  \caption{Sampling methods for system simulation with $T = 0.04s$}
  \label{fig:sampling_simulation_0.04}
\end{svg}

Forward Euler is numerically unstable for low sample rates. The bilinear
transform is a significant improvement due to it being a second-order
approximation, but zero-order hold performs best due to the matrix exponential
including much higher orders (we'll cover the matrix exponential in the next
section).

Table \ref{tab:disc_approx_scalar} compares the Taylor series expansions of the
discretization methods presented so far (these are found using polynomial
division). The bilinear transform does best with accuracy trailing off after the
third order term. Forward Euler has no second order or higher terms, so it
undershoots. Backward Euler has twice the second order term and overshoots the
remaining higher order terms as well.

\begin{booktable}
  \begin{tabular}{|cll|}
    \hline
    \rowcolor{headingbg}
    \multicolumn{1}{|c}{\textbf{Discretization method}} &
      \multicolumn{1}{c}{\textbf{Conversion}} &
      \multicolumn{1}{c|}{\textbf{Taylor series expansion}} \\
    \hline
    Zero-order hold (exact) &
      $z = e^{Ts}$ &
      $z = 1 + Ts + \frac{1}{2}T^2s^2 + \frac{1}{6}T^3s^3 + \ldots$ \\
    Bilinear transformation &
      $z = \frac{1 + \frac{1}{2}Ts}{1 - \frac{1}{2}Ts}$ &
      $z = 1 + Ts + \frac{1}{2}T^2s^2 + \frac{1}{4}T^3s^3 + \ldots$ \\
    Forward Euler &
      $z = 1 + Ts$ &
      $z = 1 + Ts$ \\
    Reverse Euler &
      $z = \frac{1}{1 - Ts}$ &
      $z = 1 + Ts + T^2s^2 + T^3s^3 + \ldots$ \\
    \hline
  \end{tabular}
  \caption{Taylor series expansions of discretization methods (scalar case)}
  \label{tab:disc_approx_scalar}
\end{booktable}

\section{The matrix exponential}
\index{Discretization!matrix exponential}

The matrix exponential (and system discretization in general) is typically
solved with a computer. Python Control's \texttt{StateSpace.sample()} with the
``zoh" method (the default) does this.

\begin{definition}[Matrix exponential]
  Let $\mtx{X}$ be an $n \times n$ matrix. The exponential of $\mtx{X}$ denoted
  by $e^{\mtx{X}}$ is the $n \times n$ matrix given by the following power
  series.

  \begin{equation}
    e^{\mtx{X}} = \sum_{k=0}^\infty \frac{1}{k!} \mtx{X}^k \label{eq:mat_exp}
  \end{equation}

  where $\mtx{X}^0$ is defined to be the identity matrix $\mtx{I}$ with the same
  dimensions as $\mtx{X}$.
\end{definition}

To understand why the matrix exponential is used in the discretization process,
consider the set of differential equations $\dot{\mtx{x}} = \mtx{A}\mtx{x}$ we
use to describe systems (systems also have a $\mtx{B}\mtx{u}$ term, but we'll
ignore it for clarity). The solution to this type of differential equation uses
an exponential. Since we are using matrices and vectors here, we use the matrix
exponential.

\begin{equation*}
  \mtx{x}(t) = e^{\mtx{A}t} \mtx{x}_0
\end{equation*}

where $\mtx{x}_0$ contains the initial conditions. If the initial state is the
current system state, then we can describe the system's state over time as

\begin{equation*}
  \mtx{x}_{k+1} = e^{\mtx{A}T} \mtx{x}_k
\end{equation*}

where $T$ is the time between samples $\mtx{x}_k$ and $\mtx{x}_{k+1}$.

\section{The Taylor series}
\index{Discretization!Taylor series}
The definition for the matrix exponential and the approximations below all use
the \textit{Taylor series expansion}. The Taylor series is a method of
approximating a function like $e^t$ via the summation of weighted polynomial
terms like $t^k$. $e^t$ has the following Taylor series around $t = 0$.

\begin{equation*}
  e^t = \sum_{n = 0}^\infty \frac{t^n}{n!}
\end{equation*}

where a finite upper bound on the number of terms produces an approximation of
$e^t$. As $n$ increases, the polynomial terms increase in power and the weights
by which they are multiplied decrease. For $e^t$ and some other functions, the
Taylor series expansion equals the original function for all values of $t$ as
the number of terms approaches infinity\footnote{Functions for which their
Taylor series expansion converges to and also equals it are called analytic
functions.}. Figure \ref{fig:taylor_series} shows the Taylor series expansion of
$e^t$ around $t = 0$ for a varying number of terms.

\begin{svg}{build/code/taylor_series}
  \caption{Taylor series expansions of $e^t$ around $t = 0$ for $n$ terms}
  \label{fig:taylor_series}
\end{svg}

We'll expand the first few terms of the Taylor series expansion in equation
(\ref{eq:mat_exp}) for $\mtx{X} = \mtx{A}T$ so we can compare it with other
methods.

\begin{equation*}
  \sum_{k=0}^3 \frac{1}{k!} (\mtx{A}T)^k = \mtx{I} + \mtx{A}T +
    \frac{1}{2}\mtx{A}^2T^2 + \frac{1}{6}\mtx{A}^3T^3
\end{equation*}

Table \ref{tab:disc_approx_matrix} compares the Taylor series expansions of the
discretization methods for the matrix case. These use a more complex formula
which we won't present here.

\begin{booktable}
  \begin{tabular}{|cll|}
    \hline
    \rowcolor{headingbg}
    \multicolumn{1}{|c}{\textbf{Discretization method}} &
      \multicolumn{1}{c}{\textbf{Conversion}} &
      \multicolumn{1}{c|}{\textbf{Taylor series expansion}} \\
    \hline
    Zero-order hold (exact) &
      $\mtx{\Phi} = e^{\mtx{A}T}$ &
      $\mtx{\Phi} = \mtx{I} + \mtx{A}T + \frac{1}{2}\mtx{A}^2T^2 +
        \frac{1}{6}\mtx{A}^3T^3 + \ldots$ \\
    Bilinear transformation &
      $\mtx{\Phi} =
        \left(\mtx{I} + \frac{1}{2}\mtx{A}T\right)
        \left(\mtx{I} - \frac{1}{2}\mtx{A}T\right)^{-1}$ &
      $\mtx{\Phi} = \mtx{I} + \mtx{A}T + \frac{1}{2}\mtx{A}^2T^2 +
        \frac{1}{4}\mtx{A}^3T^3 + \ldots$ \\
    Forward Euler &
      $\mtx{\Phi} = \mtx{I} + \mtx{A}T$ &
      $\mtx{\Phi} = \mtx{I} + \mtx{A}T$ \\
    Reverse Euler &
      $\mtx{\Phi} = \left(\mtx{I} - \mtx{A}T\right)^{-1}$ &
      $\mtx{\Phi} =
        \mtx{I} + \mtx{A}T + \mtx{A}^2T^2 + \mtx{A}^3T^3 + \ldots$ \\
    \hline
  \end{tabular}
  \caption{Taylor series expansions of discretization methods (matrix case)}
  \label{tab:disc_approx_matrix}
\end{booktable}

Each of them has different stability properties. The bilinear transform
preserves the (in)stability of the continuous-time system.

\section{Zero-order hold for state-space}
\index{Discretization!zero-order hold}

Given the following continuous-time state space model

\begin{align*}
  \dot{\mtx{x}} &= \mtx{A}_c\mtx{x} + \mtx{B}_c\mtx{u} + \mtx{w} \\
  \mtx{y} &= \mtx{C}_c\mtx{x} + \mtx{D}_c\mtx{u} + \mtx{v}
\end{align*}

where $\mtx{w}$ is the process noise, $\mtx{v}$ is the measurement noise, and
both are zero-mean white noise sources with covariances of $\mtx{Q}_c$ and
$\mtx{R}_c$ respectively

\begin{align*}
  \mtx{w} &\sim N(0, \mtx{Q}_c) \\
  \mtx{v} &\sim N(0, \mtx{R}_c)
\end{align*}

The model can be discretized as follows

\begin{align*}
  \mtx{x}_{k+1} &= \mtx{A}_d \mtx{x}_k + \mtx{B}_d \mtx{u}_k + \mtx{w}_k \\
   \mtx{y}_k &= \mtx{C}_d \mtx{x}_k + \mtx{D}_d \mtx{u}_k + \mtx{v}_k
\end{align*}

with covariances

\begin{align*}
  \mtx{w}_k &\sim N(0, \mtx{Q}_d) \\
  \mtx{v}_k &\sim N(0, \mtx{R}_d)
\end{align*}

\begin{theorem}[Zero-order hold for state-space]
  \begin{align}
    \mtx{A}_d &= e^{\mtx{A}_c T} \\
    \mtx{B}_d &= \int_0^T e^{\mtx{A}_c \tau} d\tau \mtx{B}_c =
      \mtx{A}_c^{-1} (\mtx{A}_d - \mtx{I}) \mtx{B}_c \\
    \mtx{C}_d &= \mtx{C}_c \\
    \mtx{D}_d &= \mtx{D}_c \\
    \mtx{Q}_d &= \int_{\tau = 0}^{T} e^{\mtx{A}_c\tau} \mtx{Q}_c
      e^{\mtx{A}_c^T\tau} d\tau \\
    \mtx{R}_d &= \frac{1}{T} \mtx{R}_c
  \end{align}

  where a subscript of $d$ denotes discrete, a subscript of $c$ denotes the
  continuous version of the corresponding matrix, $T$ is the sample period for
  the discrete system, and $e^{\mtx{A}_c T}$ is the matrix exponential of
  $\mtx{A}_c$.
\end{theorem}

See appendix \ref{sec:deriv-zoh-ss} for derivations.

$\mtx{Q}_d$ is computed as

\begin{equation*}
  e^{
  \begin{bmatrix}
    -\mtx{A}^T & \mtx{Q}_c \\
    \mtx{0} & \mtx{A}
  \end{bmatrix}T} =
  \begin{bmatrix}
    -\mtx{A}_d^T & \mtx{A}_d^{-1} \mtx{Q}_d \\
    \mtx{0} & \mtx{A}_d
  \end{bmatrix}
\end{equation*}

and $\mtx{Q}_d$ is the lower-right quadrant multiplied by the upper-right
quadrant \cite{bib:integral_matrix_exp}. To compute $\mtx{A}_d$ and $\mtx{B}_d$
in one step, one can utilize the following property.

\begin{equation*}
  e^{
  \begin{bmatrix}
    \mtx{A} & \mtx{B} \\
    \mtx{0} & \mtx{0}
  \end{bmatrix}T} =
  \begin{bmatrix}
    \mtx{A}_d & \mtx{B}_d \\
    \mtx{0} & \mtx{I}
  \end{bmatrix}
\end{equation*}
