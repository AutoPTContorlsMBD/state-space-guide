\chapterimage{ss-representation.jpg}{Road near walking trail off of Rice Ranch Road in Santa Maria, CA}

\chapter{State-space representation}

\begin{remark}
  Chapters from here on use Python Control to demonstrate the concepts discussed
  and perform the complex math required. See appendix
  \ref{ch:app-installing-python-control} for how to install it.
\end{remark}

State-space representation models \glspl{system} as a set of \gls{state}, input,
and output variables related by first-order differential equations that describe
how the system's state changes over time given the current \glspl{state} and
inputs.

\section{Benefits over classical output-based control}

The state-space method provides a more convenient and compact way to model and
analyze \glspl{system} with multiple inputs and outputs. For a system with $p$
inputs and $q$ outputs, we would have to write $q \times p$ Laplace transforms
to represent it. Not only is the resulting algebra unwieldy, but it only works
for linear systems with zero initial conditions. State-space representation uses
the time domain instead of the Laplace domain, so it doesn't have this problem.

Students are still taught classical control first because it provides a
framework within which to understand the results we get from the fancy
mathematical machinery of modern control.

\section{What is state-space?}

Recall from last chapter that 2D space has two axes, $x$ and $y$. We represent
locations within this space as a pair of numbers packaged in a vector, and each
coordinate is a measure of how far to move along the corresponding axis.
State-space is a Cartesian coordinate system with an axis for each \gls{state}
variable, and we represent locations within it the same way we do for 2D space:
with a list of numbers in a vector. Each element in the vector corresponds to a
\gls{state} of the \gls{system}.

In addition to the \gls{state}, inputs and outputs are represented as vectors.
Since the mapping from the current states and inputs to the change in state is a
system of equations, it's natural to write it in matrix form.

\section{State-space notation}

Below are the continuous and discrete versions of state-space notation.

\begin{align}
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{u} \label{eq:ss_ctrl_x} \\
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}\mtx{u} \label{eq:ss_ctrl_y}
\end{align}

\begin{align}
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k \label{eq:ssz_ctrl_x} \\
  \mtx{y}_{k+1} &= \mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k \label{eq:ssz_ctrl_y}
\end{align}

\begin{center}
  \renewcommand{\arraystretch}{1.3}
  \begin{tabulary}{\linewidth}{LLLL}
    $\mtx{A}$ & system matrix      & $\mtx{x}$ & state vector \\
    $\mtx{B}$ & input matrix       & $\mtx{u}$ & input vector \\
    $\mtx{C}$ & output matrix      & $\mtx{y}$ & output vector \\
    $\mtx{D}$ & feedthrough matrix &  &  \\
  \end{tabulary}
\end{center}

\begin{table}[h]
  \renewcommand{\arraystretch}{1.5}
  \centering
  \begin{tabular}{|ll|ll|}
    \hline
    \rowcolor{headingbg}
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} &
    \textbf{Matrix} & \textbf{Rows $\times$ Columns} \\
    \hline
    $\mtx{A}$ & states $\times$ states & $\mtx{x}$ & states $\times$ 1 \\
    $\mtx{B}$ & states $\times$ inputs & $\mtx{u}$ & inputs $\times$ 1 \\
    $\mtx{C}$ & outputs $\times$ states & $\mtx{y}$ & outputs $\times$ 1 \\
    $\mtx{D}$ & outputs $\times$ inputs &  &  \\
    \hline
  \end{tabular}
  \caption{State-space matrix dimensions}
  \label{tab:ss_matrix_dims}
\end{table}

In the continuous case, the change in state and the output are linear
combinations of the state vector and the input vector. The $\mtx{A}$, $\mtx{B}$,
$\mtx{C}$, and $\mtx{D}$ matrices are used to map the state vector $\mtx{x}$ and
the input vector $\mtx{u}$ to a change in state.

\section{Canonical forms}

There are two canonical forms used to represent state-space \glspl{model}:
controllable canonical form and observable canonical form. They are used to
provide controllability and observability of a system respectively, which are
mathematical duals of each other. That is, the controller and estimator (state
observer) are complementary problems.

\subsection{Controllable canonical form} \label{subsubsec:ctrl-canon}

State controllability implies that it is possible -- by admissible inputs -- to
steer the \glspl{state} from any initial value to any final value within some
finite time window.

\begin{theorem}[Controllable canonical form]
  A continuous \gls{time-invariant} linear state-space \gls{model} is
  controllable if and only if

  \begin{equation}
    \text{rank} \left(
    \begin{bmatrix}
      \mtx{B} & \mtx{A}\mtx{B} & \mtx{A}^2\mtx{B} & \cdots &
      \mtx{A}^{n-1}\mtx{B}
    \end{bmatrix}
    \right) = n
    \label{eq:ctrl_rank}
  \end{equation}

  where rank is the number of linearly independent rows in a matrix and $n$ is
  the number of \gls{state} variables.
\end{theorem}

Given a \gls{system} of the form

\begin{equation} \label{eq:ctrl_obsv_tf}
  G(s) = \frac{n_1 s^3 + n_2 s^2 + n_3 s + n_4}
    {s^4 + d_1 s^3 + d_2 s^2 + d_3 s + d_4}
\end{equation}

the canonical \gls{realization} of it that satisfies equation
(\ref{eq:ctrl_rank}) is

\begin{align}
  \dot{\mtx{x}}(t) &=
  \begin{bmatrix}
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
    -d_4 & -d_3 & -d_2 & -d_1
  \end{bmatrix}
  \mtx{x}(t) +
  \begin{bmatrix}
    0 \\
    0 \\
    0 \\
    1
  \end{bmatrix}
  \mtx{u}(t) \\
  \mtx{y}(t) &=
  \begin{bmatrix}
    n_4 & n_3 & n_2 & n_1
  \end{bmatrix}
  \mtx{x}(t)
\end{align}

\subsection{Observable canonical form} \label{subsubsec:obsv-canon}

Observability is a measure for how well internal \glspl{state} of a \gls{system}
can be inferred by knowledge of its external outputs. The observability and
controllability of a \gls{system} are mathematical duals (i.e., as
controllability proves that an input is available that brings any initial
\gls{state} to any desired final \gls{state}, observability proves that knowing
an output trajectory provides enough information to predict the initial
\gls{state} of the \gls{system}).

\begin{theorem}[Observable canonical form]
  A continuous \gls{time-invariant} linear state-space \gls{model} is observable
  if and only if

  \begin{equation} \label{eq:obsv_rank}
    \text{rank} \left(
    \begin{bmatrix}
      C \\
      CA \\
      \vdots \\
      CA^{n-1}
    \end{bmatrix}\right) = n
  \end{equation}

  where rank is the number of linearly independent rows in a matrix and $n$ is
  the number of \gls{state} variables.
\end{theorem}

The canonical \gls{realization} of the \gls{system} in equation
(\ref{eq:ctrl_obsv_tf}) that satisfies equation (\ref{eq:obsv_rank}) is

\begin{align}
  \dot{\mtx{x}}(t) &=
  \begin{bmatrix}
    0 & 0 & 0 & -d_4 \\
    1 & 0 & 0 & -d_3 \\
    0 & 1 & 0 & -d_2 \\
    0 & 0 & 1 & -d_1
  \end{bmatrix}
  \mtx{x}(t) +
  \begin{bmatrix}
    n_4 \\
    n_3 \\
    n_2 \\
    n_1
  \end{bmatrix}
  \mtx{u}(t) \\
  \mtx{y}(t) &=
  \begin{bmatrix}
    0 & 0 & 0 & 1
  \end{bmatrix}
  \mtx{x}(t)
\end{align}

\section{Eigenvalues in state-space}

The eigenvalues of the system matrix can be used to determine the stability of a
\gls{system}.

We'd like to know whether the \gls{system} defined by equation
(\ref{eq:ssz_ctrl_x}) operating with the \gls{control law}
$\mtx{u}_k = \mtx{K}(\mtx{r}_k - \mtx{x}_k)$ converges to the \gls{reference}
$\mtx{r}_k$.

\begin{align}
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k \nonumber \\
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}(\mtx{K}(\mtx{r}_k - \mtx{x}_k))
    \nonumber \\
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{K}\mtx{r}_k -
    \mtx{B}\mtx{K}\mtx{x}_k \nonumber \\
  \mtx{x}_{k+1} &= \mtx{A}\mtx{x}_k - \mtx{B}\mtx{K}\mtx{x}_k +
    \mtx{B}\mtx{K}\mtx{r}_k \nonumber \\
  \mtx{x}_{k+1} &= (\mtx{A} - \mtx{B}\mtx{K})\mtx{x}_k +
    \mtx{B}\mtx{K}\mtx{r}_k \label{eq:ctrl_eig_calc}
\end{align}

For equation (\ref{eq:ctrl_eig_calc}) to have a bounded output, the eigenvalues
of $\mtx{A} - \mtx{B}\mtx{K}$ must be within the unit circle.

This derivation can be performed for a \gls{state} estimator as well to
determine whether the \gls{state} estimate converges to the true \gls{state}.
Plugging equation (\ref{eq:z_obsv_y}) into equation (\ref{eq:z_obsv_x}) gives

\begin{align*}
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - \hat{\mtx{y}}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - (\mtx{C}\hat{\mtx{x}}_k + \mtx{D}\mtx{u}_k)) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - \mtx{C}\hat{\mtx{x}}_k - \mtx{D}\mtx{u}_k)
\end{align*}

Plugging in equation (\ref{eq:ssz_ctrl_y}) gives

\begin{align*}
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}((\mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k) - \mtx{C}\hat{\mtx{x}}_k -
    \mtx{D}\mtx{u}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}(\mtx{C}\mtx{x}_k + \mtx{D}\mtx{u}_k - \mtx{C}\hat{\mtx{x}}_k -
    \mtx{D}\mtx{u}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}(\mtx{C}\mtx{x}_k - \mtx{C}\hat{\mtx{x}}_k) \\
  \hat{\mtx{x}}_{k+1} &= \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}\mtx{C}(\mtx{x}_k - \hat{\mtx{x}}_k)
\end{align*}

Let $E_k = \mtx{x}_k - \hat{\mtx{x}}_k$ be the error in the estimate
$\hat{\mtx{x}}_k$.

\begin{equation*}
  \hat{\mtx{x}}_{k+1} = \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L}\mtx{C}\mtx{E}_k
\end{equation*}

Subtracting this from equation (\ref{eq:ssz_ctrl_x}) gives

\begin{align}
  \mtx{x}_{k+1} - \hat{\mtx{x}}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k -
    (\mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
     \mtx{L}\mtx{C}\mtx{E}_k) \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k -
    (\mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k + \mtx{L}\mtx{C}\mtx{E}_k)
    \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{x}_k + \mtx{B}\mtx{u}_k -
    \mtx{A}\hat{\mtx{x}}_k - \mtx{B}\mtx{u}_k - \mtx{L}\mtx{C}\mtx{E}_k
    \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{x}_k - \mtx{A}\hat{\mtx{x}}_k -
    \mtx{L}\mtx{C}\mtx{E}_k \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}(\mtx{x}_k - \hat{\mtx{x}}_k) -
    \mtx{L}\mtx{C}\mtx{E}_k \nonumber \\
  \mtx{E}_{k+1} &= \mtx{A}\mtx{E}_k - \mtx{L}\mtx{C}\mtx{E}_k \nonumber \\
  \mtx{E}_{k+1} &= (\mtx{A} - \mtx{L}\mtx{C})\mtx{E}_k \label{eq:obsv_eig_calc}
\end{align}

For equation (\ref{eq:obsv_eig_calc}) to have a bounded output, the eigenvalues
of $\mtx{A} - \mtx{L}\mtx{C}$ must be within the unit circle. These eigenvalues
represent how fast the estimator converges to the true state of the given
\gls{model}. A fast estimator converges quickly while a slow estimator avoids
amplifying noise in the measurements used to produce a state estimate.

The effect of noise can be seen if it is modeled
\glslink{stochastic process}{stochastically} as

\begin{equation*}
  \hat{\mtx{x}}_{k+1} = \mtx{A}\hat{\mtx{x}}_k + \mtx{B}\mtx{u}_k +
    \mtx{L} (\mtx{y}_k - \hat{\mtx{y}}_k) + \mtx{L}\mtx{\nu}_k
\end{equation*}

where $\mtx{\nu}_k$ is the measurement noise. As $\mtx{L}$ increases, the
measurement noise is amplified.

In summary, a controller is stable if the eigenvalues of
$\mtx{A} - \mtx{B}\mtx{K}$ are within the unit circle, and an estimator is
stable if the eigenvalues of $\mtx{A} - \mtx{L}\mtx{C}$ are within the unit
circle.

As stated before, the controller and estimator are dual problems. Controller
gains can be found assuming perfect estimator (i.e., perfect knowledge of all
\glspl{state}). Estimator gains can be found assuming an accurate \gls{model}
and a controller with perfect \gls{tracking}.
