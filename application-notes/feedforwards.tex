\section{Feedforwards}

\subsection{Steady-state feedforward}

$\mtx{N}_x$ converts a desired output $\mtx{y}_c$ to a desired state
$\mtx{x}_c$ (also known as $\mtx{r}$).

\begin{equation*}
  \mtx{x}_c = \mtx{N}_x\mtx{y}_c
\end{equation*}

For steady-state, that is

\begin{equation*}
  \mtx{x}_{ss} = \mtx{N}_x\mtx{y}_{ss}
\end{equation*}

$\mtx{N}_u$ converts the desired output $\mtx{y}$ to the control input
$\mtx{u}$ required at steady-state.

\begin{equation*}
  \mtx{u}_c = \mtx{N}_u\mtx{y}_c
\end{equation*}

For steady-state, that is

\begin{equation*}
  \mtx{u}_{ss} = \mtx{N}_u\mtx{y}_{ss}
\end{equation*}

To find the control input required at steady-state, set equation
(\ref{eq:ss_ctrl_x}) to zero.

\begin{align*}
  \dot{\mtx{x}} &= \mtx{A}\mtx{x} + \mtx{B}\mtx{u} \\
  \mtx{y} &= \mtx{C}\mtx{x} + \mtx{D}\mtx{u}
\end{align*}

\begin{align*}
  \mtx{0} &= \mtx{A}\mtx{x}_{ss} + \mtx{B}\mtx{u}_{ss} \\
  \mtx{y}_{ss} &= \mtx{C}\mtx{x}_{ss} + \mtx{D}\mtx{u}_{ss}
\end{align*}

\begin{align*}
  \mtx{0} &= \mtx{A}\mtx{N}_x\mtx{y}_{ss} + \mtx{B}\mtx{N}_u\mtx{y}_{ss} \\
  \mtx{y}_{ss} &= \mtx{C}\mtx{N}_x\mtx{y}_{ss} + \mtx{D}\mtx{N}_u\mtx{y}_{ss}
\end{align*}

\begin{align*}
  \begin{bmatrix}
    \mtx{0} \\
    \mtx{y}_{ss}
  \end{bmatrix} &=
  \begin{bmatrix}
    \mtx{A}\mtx{N}_x + \mtx{B}\mtx{N}_u \\
    \mtx{C}\mtx{N}_x + \mtx{D}\mtx{N}_u
  \end{bmatrix}
  \mtx{y}_{ss} \\
  \begin{bmatrix}
    \mtx{0} \\
    \mtx{1}
  \end{bmatrix} &=
  \begin{bmatrix}
    \mtx{A}\mtx{N}_x + \mtx{B}\mtx{N}_u \\
    \mtx{C}\mtx{N}_x + \mtx{D}\mtx{N}_u
  \end{bmatrix} \\
  \begin{bmatrix}
    \mtx{0} \\
    \mtx{1}
  \end{bmatrix} &=
  \begin{bmatrix}
    \mtx{A} & \mtx{B} \\
    \mtx{C} & \mtx{D}
  \end{bmatrix}
  \begin{bmatrix}
    \mtx{N}_x \\
    \mtx{N}_u
  \end{bmatrix}
\end{align*}

\begin{theorem}[Steady-state feedforward]
  \begin{align}
    &\begin{bmatrix}
      \mtx{N}_x \\
      \mtx{N}_u
    \end{bmatrix} =
    \begin{bmatrix}
      \mtx{A} & \mtx{B} \\
      \mtx{C} & \mtx{D}
    \end{bmatrix}^{\dagger}
    \begin{bmatrix}
      \mtx{0} \\
      \mtx{1}
    \end{bmatrix} \\
    &\text{where $^\dagger$ is the Moore-Penrose pseudoinverse.} \nonumber
  \end{align}

  In the augmented matrix, $\mtx{B}$ should contain one column corresponding to
  an actuator and $\mtx{C}$ should contain one row whose output will be driven
  by that actuator. More than one actuator or output can be included in the
  computation at once, but the result won't be the same as if they were computed
  independently and summed afterward.

  If the augmented matrix is square (number of inputs = number of outputs), the
  normal inverse can be used instead.
\end{theorem}

After computing the feedforward for each actuator-output pair, the respective
collections of $\mtx{N}_x$ and $\mtx{N}_u$ matrices can be summed to produce
combined versions of each.

\subsection{Two-state feedforward}

Let's start with the equation for the reference dynamics

\begin{equation*}
  \mtx{r}_{k+1} = \mtx{A}\mtx{r}_k + \mtx{B}\mtx{u}_{ff}
\end{equation*}

where $\mtx{u}_{ff}$ is the feedforward input. Note that this feedforward
equation does not and should not take into account any feedback terms. We want
to find the optimal $\mtx{u}_{ff}$ such that we minimize the tracking error
between $\mtx{r}_{k+1}$ and $\mtx{r}_k$.

\begin{equation*}
  \mtx{r}_{k+1} - \mtx{A}\mtx{r}_k = \mtx{B}\mtx{u}_{ff}
\end{equation*}

To solve for $\mtx{u}_{ff}$, we need to take the inverse of the nonsquare matrix
$\mtx{B}$. This isn't possible, but we can find the pseudoinverse given some
constraints on the state tracking error and control effort. To find the optimal
solution for these sorts of trade-offs, one can define a cost function and
attempt to minimize it. To do this, we'll first solve the expression for
$\mtx{0}$.

\begin{equation*}
  \mtx{0} = \mtx{B}\mtx{u}_{ff} - (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k)
\end{equation*}

This expression will be the state tracking cost we use in our cost function.

Our cost function will use an $H_2$ norm with $\mtx{Q}$ as the state cost matrix
with dimensionality $states \times states$ and $\mtx{R}$ as the control input
cost matrix with dimensionality $inputs \times inputs$.

\begin{equation*}
  \mtx{J} = (\mtx{B}\mtx{u}_{ff} - (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k))^T \mtx{Q}
    (\mtx{B}\mtx{u}_{ff} - (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k)) +
    \mtx{u}_{ff}^T\mtx{R}\mtx{u}_{ff}
\end{equation*}

\begin{remark}
  $\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k$ will only return a nonzero vector if the
  reference isn't following the system dynamics. If it is, the feedback
  controller already compensates for it. This feedforward compensates for any
  unmodeled dynamics reflected in how the reference is changing (or not
  changing). In the case of a constant reference, the feedforward opposes any
  system dynamics that would change the state over time.
\end{remark}

The following thorems will be needed to find the minimum of $\mtx{J}$.

\begin{theorem}
  $\frac{\partial \mtx{x}^T\mtx{A}\mtx{x}}{\partial\mtx{x}} =
    2\mtx{A}\mtx{x}$ where $\mtx{A}$ is symmetric.
  \label{thm:partial_xax}
\end{theorem}

\begin{theorem}
  $\frac{\partial (\mtx{A}\mtx{x} + \mtx{b})^T\mtx{C}
    (\mtx{D}\mtx{x} + \mtx{e})}{\partial\mtx{x}} =
    \mtx{A}^T\mtx{C}(\mtx{D}\mtx{x} + \mtx{e}) + \mtx{D}^T\mtx{C}^T
    (\mtx{A}\mtx{x} + \mtx{b})$
  \label{thm:partial_ax_b}
\end{theorem}

\begin{corollary}
  $\frac{\partial (\mtx{A}\mtx{x} + \mtx{b})^T\mtx{C}
    (\mtx{A}\mtx{x} + \mtx{b})}{\partial\mtx{x}} =
    2\mtx{A}^T\mtx{C}(\mtx{A}\mtx{x} + \mtx{b})$ where $\mtx{C}$ is symmetric.
  \label{cor:partial_ax_b}

  Proof:
  \begin{align*}
    \frac{\partial (\mtx{A}\mtx{x} + \mtx{b})^T\mtx{C}
      (\mtx{A}\mtx{x} + \mtx{b})}{\partial\mtx{x}} &=
      \mtx{A}^T\mtx{C}(\mtx{A}\mtx{x} + \mtx{b}) + \mtx{A}^T\mtx{C}^T
      (\mtx{A}\mtx{x} + \mtx{b}) \\
    \frac{\partial (\mtx{A}\mtx{x} + \mtx{b})^T\mtx{C}
      (\mtx{A}\mtx{x} + \mtx{b})}{\partial\mtx{x}} &=
      (\mtx{A}^T\mtx{C} + \mtx{A}^T\mtx{C}^T)(\mtx{A}\mtx{x} + \mtx{b})
  \end{align*}

  $\mtx{C}$ is symmetric, so

  \begin{align*}
    \frac{\partial (\mtx{A}\mtx{x} + \mtx{b})^T\mtx{C}
      (\mtx{A}\mtx{x} + \mtx{b})}{\partial\mtx{x}} &=
      (\mtx{A}^T\mtx{C} + \mtx{A}^T\mtx{C})(\mtx{A}\mtx{x} + \mtx{b}) \\
    \frac{\partial (\mtx{A}\mtx{x} + \mtx{b})^T\mtx{C}
      (\mtx{A}\mtx{x} + \mtx{b})}{\partial\mtx{x}} &=
      2\mtx{A}^T\mtx{C}(\mtx{A}\mtx{x} + \mtx{b})
  \end{align*}
\end{corollary}

Given theorem \ref{thm:partial_xax} and corollary \ref{cor:partial_ax_b}, find
the minimum of $\mtx{J}$ by taking the partial derivative with respect to
$\mtx{u}_{ff}$ and setting the result to $\mtx{0}$.

\begin{align*}
  \frac{\partial\mtx{J}}{\partial\mtx{u}_{ff}} &= 2\mtx{B}^T\mtx{Q}
    (\mtx{B}\mtx{u}_{ff} - (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k)) +
    2\mtx{R}\mtx{u}_{ff} \\
  \mtx{0} &= 2\mtx{B}^T\mtx{Q}
    (\mtx{B}\mtx{u}_{ff} - (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k)) +
    2\mtx{R}\mtx{u}_{ff} \\
  \mtx{0} &= \mtx{B}^T\mtx{Q}
    (\mtx{B}\mtx{u}_{ff} - (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k)) +
    \mtx{R}\mtx{u}_{ff} \\
  \mtx{0} &= \mtx{B}^T\mtx{Q}\mtx{B}\mtx{u}_{ff} -
    \mtx{B}^T\mtx{Q}(\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k) + \mtx{R}\mtx{u}_{ff} \\
  \mtx{B}^T\mtx{Q}\mtx{B}\mtx{u}_{ff} + \mtx{R}\mtx{u}_{ff} &=
    \mtx{B}^T\mtx{Q}(\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k) \\
  (\mtx{B}^T\mtx{Q}\mtx{B} + \mtx{R})\mtx{u}_{ff} &=
    \mtx{B}^T\mtx{Q}(\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k) \\
  \mtx{u}_{ff} &= (\mtx{B}^T\mtx{Q}\mtx{B} + \mtx{R})^{-1}
    \mtx{B}^T\mtx{Q}(\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k)
\end{align*}

\begin{theorem}[Two-state feedforward]
  \begin{align}
    &\mtx{u}_{ff} = \mtx{K}_{ff} (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k) \\
    &\text{where } \mtx{K}_{ff} =
      (\mtx{B}^T\mtx{Q}\mtx{B} + \mtx{R})^{-1}\mtx{B}^T\mtx{Q}
  \end{align}
  \label{thm:two-state_ff}
\end{theorem}

If control effort is considered inexpensive, $\mtx{R} \ll \mtx{Q}$ and
$\mtx{u}_{ff}$ approaches corollary \ref{cor:two-state_ff_no_r}.

\begin{corollary}[Two-state feedforward with inexpensive control effort]
  \begin{align}
    &\mtx{u}_{ff} = \mtx{K}_{ff} (\mtx{r}_{k+1} - \mtx{A}\mtx{r}_k) \\
    &\text{where } \mtx{K}_{ff} =
      (\mtx{B}^T\mtx{Q}\mtx{B})^{-1}\mtx{B}^T\mtx{Q}
  \end{align}
  \label{cor:two-state_ff_no_r}
\end{corollary}

\begin{remark}
  If the cost matrix $\mtx{Q}$ isn't included in the cost function (that is,
  $\mtx{Q}$ is set to the identity matrix), $\mtx{K}_{ff}$ becomes the
  Moore-Penrose pseudoinverse of $\mtx{B}$ given by
  $\mtx{B}^\dagger = (\mtx{B}^T\mtx{B})^{-1}\mtx{B}^T$.
\end{remark}
